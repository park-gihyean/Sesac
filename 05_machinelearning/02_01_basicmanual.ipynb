{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2172d61e-09b3-4ade-96c8-9c68d694a8ce",
   "metadata": {},
   "source": [
    "\n",
    "사이킷 런\n",
    "\n",
    "사이킷 런은 파이썬 머신러닝 라이브러리 중 가장 기초적이고 가장 많이 사용되는 라이브러리이다.\n",
    "\n",
    "사이킷런 스타일의 API 는 다른 머신러닝 패키지도 채탁하고 있다.\n",
    "\n",
    "사이킷 런에서는 예제용 데이터 셋도 제공하고 있다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5acfda36-8837-44e4-aed4-0742be38c46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.13\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6dd2af6-8af8-4171-a3df-e8cddff5b34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "608909d0-8dff-4240-bb08-6f0f6c8f5e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c544e1f-587f-4a6e-85bb-1b5e22a1b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn 에서 제공하는 기본 데이터셋\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# 분류 모델\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 회귀 모델\n",
    "#from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Decision 모델을 이용하여 분류 혹은 회귀에 적용 할 수 있다.\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "159f4f59-f734-4c8b-bd43-a0172d4bae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "418f7153-463c-4c71-88a4-2ead93c50173",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'iris.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_iris() # shift + tab"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d84e2478-26d3-4897-82d5-b0b7cd43f8cb",
   "metadata": {},
   "source": [
    "설명\n",
    " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
    "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
    "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
    " 'frame': None,\n",
    " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
    "\n",
    "각 인덱스의 의미\n",
    "\n",
    "\n",
    "'feature_names': ['sepal length (cm)',\n",
    "  'sepal width (cm)',\n",
    "  'petal length (cm)',\n",
    "  'petal width (cm)'],\n",
    "\n",
    "각 컬럼의 의미\n",
    "\n",
    "딕션어리 형태의 자료형을 반환한다.\n",
    "data 키에 데이터(리스트)_리스트, target 키에 결과_리스트를 가지고 있다.\n",
    "target의 0, 1, 2 는 target_names  키의 결과명_리스트의 인덱스값으로 매칭된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef82cc75-7fda-49fd-af6a-2ed02d127e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris(as_frame = True) # 데이터 프레임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28ff4e4a-c996-49a0-956e-8f14ede32352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data':      sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       " 0                  5.1               3.5                1.4               0.2\n",
       " 1                  4.9               3.0                1.4               0.2\n",
       " 2                  4.7               3.2                1.3               0.2\n",
       " 3                  4.6               3.1                1.5               0.2\n",
       " 4                  5.0               3.6                1.4               0.2\n",
       " ..                 ...               ...                ...               ...\n",
       " 145                6.7               3.0                5.2               2.3\n",
       " 146                6.3               2.5                5.0               1.9\n",
       " 147                6.5               3.0                5.2               2.0\n",
       " 148                6.2               3.4                5.4               2.3\n",
       " 149                5.9               3.0                5.1               1.8\n",
       " \n",
       " [150 rows x 4 columns],\n",
       " 'target': 0      0\n",
       " 1      0\n",
       " 2      0\n",
       " 3      0\n",
       " 4      0\n",
       "       ..\n",
       " 145    2\n",
       " 146    2\n",
       " 147    2\n",
       " 148    2\n",
       " 149    2\n",
       " Name: target, Length: 150, dtype: int32,\n",
       " 'frame':      sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       " 0                  5.1               3.5                1.4               0.2   \n",
       " 1                  4.9               3.0                1.4               0.2   \n",
       " 2                  4.7               3.2                1.3               0.2   \n",
       " 3                  4.6               3.1                1.5               0.2   \n",
       " 4                  5.0               3.6                1.4               0.2   \n",
       " ..                 ...               ...                ...               ...   \n",
       " 145                6.7               3.0                5.2               2.3   \n",
       " 146                6.3               2.5                5.0               1.9   \n",
       " 147                6.5               3.0                5.2               2.0   \n",
       " 148                6.2               3.4                5.4               2.3   \n",
       " 149                5.9               3.0                5.1               1.8   \n",
       " \n",
       "      target  \n",
       " 0         0  \n",
       " 1         0  \n",
       " 2         0  \n",
       " 3         0  \n",
       " 4         0  \n",
       " ..      ...  \n",
       " 145       2  \n",
       " 146       2  \n",
       " 147       2  \n",
       " 148       2  \n",
       " 149       2  \n",
       " \n",
       " [150 rows x 5 columns],\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'iris.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris # 데이터 프레임 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4e84906-a680-4ccd-a710-f08d1006aff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d5a5053-c477-4afe-af96-efa3c552b8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                  5.1               3.5                1.4               0.2\n",
       "1                  4.9               3.0                1.4               0.2\n",
       "2                  4.7               3.2                1.3               0.2\n",
       "3                  4.6               3.1                1.5               0.2\n",
       "4                  5.0               3.6                1.4               0.2\n",
       "..                 ...               ...                ...               ...\n",
       "145                6.7               3.0                5.2               2.3\n",
       "146                6.3               2.5                5.0               1.9\n",
       "147                6.5               3.0                5.2               2.0\n",
       "148                6.2               3.4                5.4               2.3\n",
       "149                5.9               3.0                5.1               1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54ade14d-f256-4c9d-9e32-9dae97d5976c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93142277-3a9f-4405-88bf-f4d6f8f13b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "145    2\n",
       "146    2\n",
       "147    2\n",
       "148    2\n",
       "149    2\n",
       "Name: target, Length: 150, dtype: int32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c049a66b-496e-4076-8d46-26e405e06766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c876b11-893b-4101-8910-fd389028686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71513f34-6ddc-4f22-9ed1-65be1fefe089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iris_df['label'] = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f768f136-4ebb-4c96-a24a-c712b4816116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "653cd234-d9f3-407e-866d-65c18e10cdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'setosa'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04da271a-44d1-4f55-b92e-84b42366efb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       " 0                  5.1               3.5                1.4               0.2\n",
       " 120                6.9               3.2                5.7               2.3\n",
       " 122                7.7               2.8                6.7               2.0\n",
       " 49                 5.0               3.3                1.4               0.2\n",
       " 29                 4.7               3.2                1.6               0.2\n",
       " ..                 ...               ...                ...               ...\n",
       " 76                 6.8               2.8                4.8               1.4\n",
       " 13                 4.3               3.0                1.1               0.1\n",
       " 81                 5.5               2.4                3.7               1.0\n",
       " 91                 6.1               3.0                4.6               1.4\n",
       " 80                 5.5               2.4                3.8               1.1\n",
       " \n",
       " [120 rows x 4 columns],\n",
       "      sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       " 112                6.8               3.0                5.5               2.1\n",
       " 145                6.7               3.0                5.2               2.3\n",
       " 133                6.3               2.8                5.1               1.5\n",
       " 56                 6.3               3.3                4.7               1.6\n",
       " 111                6.4               2.7                5.3               1.9\n",
       " 9                  4.9               3.1                1.5               0.1\n",
       " 65                 6.7               3.1                4.4               1.4\n",
       " 15                 5.7               4.4                1.5               0.4\n",
       " 30                 4.8               3.1                1.6               0.2\n",
       " 63                 6.1               2.9                4.7               1.4\n",
       " 119                6.0               2.2                5.0               1.5\n",
       " 62                 6.0               2.2                4.0               1.0\n",
       " 84                 5.4               3.0                4.5               1.5\n",
       " 113                5.7               2.5                5.0               2.0\n",
       " 139                6.9               3.1                5.4               2.1\n",
       " 41                 4.5               2.3                1.3               0.3\n",
       " 103                6.3               2.9                5.6               1.8\n",
       " 66                 5.6               3.0                4.5               1.5\n",
       " 110                6.5               3.2                5.1               2.0\n",
       " 142                5.8               2.7                5.1               1.9\n",
       " 69                 5.6               2.5                3.9               1.1\n",
       " 2                  4.7               3.2                1.3               0.2\n",
       " 22                 4.6               3.6                1.0               0.2\n",
       " 51                 6.4               3.2                4.5               1.5\n",
       " 12                 4.8               3.0                1.4               0.1\n",
       " 11                 4.8               3.4                1.6               0.2\n",
       " 149                5.9               3.0                5.1               1.8\n",
       " 75                 6.6               3.0                4.4               1.4\n",
       " 16                 5.4               3.9                1.3               0.4\n",
       " 85                 6.0               3.4                4.5               1.6,\n",
       " 0      0\n",
       " 120    2\n",
       " 122    2\n",
       " 49     0\n",
       " 29     0\n",
       "       ..\n",
       " 76     1\n",
       " 13     0\n",
       " 81     1\n",
       " 91     1\n",
       " 80     1\n",
       " Name: target, Length: 120, dtype: int32,\n",
       " 112    2\n",
       " 145    2\n",
       " 133    2\n",
       " 56     1\n",
       " 111    2\n",
       " 9      0\n",
       " 65     1\n",
       " 15     0\n",
       " 30     0\n",
       " 63     1\n",
       " 119    2\n",
       " 62     1\n",
       " 84     1\n",
       " 113    2\n",
       " 139    2\n",
       " 41     0\n",
       " 103    2\n",
       " 66     1\n",
       " 110    2\n",
       " 142    2\n",
       " 69     1\n",
       " 2      0\n",
       " 22     0\n",
       " 51     1\n",
       " 12     0\n",
       " 11     0\n",
       " 149    2\n",
       " 75     1\n",
       " 16     0\n",
       " 85     1\n",
       " Name: target, dtype: int32]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습, 테스트용 데이터로 분리\n",
    "train_test_split(iris.data,iris.target,test_size=0.2, random_state=11)\n",
    "# iris.data 데이터, iris.target 결과 값,  순서가 동일하게 있음, 따로 따로 순서가 섞이지 않게 주의\n",
    " # test_size=None,\n",
    " # random_state=None,\n",
    " # shuffle=True, 학습 시 데이터를 섞어서\n",
    " # stratify=None,데이터를 분류를 동등하게 넣을 것인가."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c27f89c3-6ca4-436e-b3c4-2d6f32ad16c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습, 테스트 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data,iris.target,test_size=0.2, random_state=11)\n",
    "\n",
    "# test_size = 0.2  전체 데이터의 0.2 를 테스트 용으로, 0.8 를 학습용으로 사용한다.\n",
    "# 8:2           8:2\n",
    "# random_state 일종의 시드값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "531e8b3c-7f51-4e3e-bf8c-b6dacda6f7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 30, 120, 30)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1321586-304d-47cf-8f74-22c9141ae859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "dt_clf = DecisionTreeClassifier(random_state=11)\n",
    "# 모델 자체적으로도 데이터를 순서를 랜덤으로 결정하는 동작을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b4f71bf-4fbd-43dd-8313-e25d3707ea33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=11)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f3847e9-66e0-4a7a-86ef-fce4403c337c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=11)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습\n",
    "dt_clf.fit(X_train,y_train)\n",
    "# 학습 데이터 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad79933d-df6b-4da3-9b63-d8abbb2873c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 예측\n",
    "pred = dt_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fb0577b-59da-41cb-aadb-edaacb5fff0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 2),\n",
       " (2, 2),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (2, 2),\n",
       " (0, 0),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (0, 0),\n",
       " (1, 1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(pred,y_test))\n",
    "# zip 함수???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e56a336-29df-40f1-8c93-b5ebcd92198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11b25d37-772f-45ab-8717-386e11045297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,pred)\n",
    "# 정확도 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a962fb7-08a1-4bfe-a72d-76060102df63",
   "metadata": {},
   "source": [
    "제공하는 데이트 말고 사용할 데이터를 어떻게 가져오고 전처리 할 것인가???\n",
    "모든 데이터를 수치화 해야한다.\n",
    "글자를 수치화 하는 작업 -> 자연어 처리 과정\n",
    "숫자값으로만 작업이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d70da8b-0546-41bd-97b6-baa5f75f4a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\park\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dt_clf.predict([4.67,3.9,2,0.4]) 이차원 데이터가 필요하다\n",
    "dt_clf.predict([[4.67,3.9,2,0.4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3bcff3b2-27c8-4538-86ec-a899532e2103",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8125\n",
      "0.8125\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.8125\n",
      "0.8125\n",
      "0.8125\n",
      "0.8125\n",
      "0.8125\n",
      "0.8125\n",
      "0.8125\n",
      "0.8125\n",
      "0.8125\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.8125\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.8125\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.8125\n",
      "0.9375\n",
      "0.8125\n",
      "0.8125\n",
      "0.9375\n",
      "0.8125\n",
      "0.9375\n",
      "0.9375\n",
      "0.8125\n",
      "0.9375\n",
      "0.9375\n",
      "0.8125\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.8125\n",
      "0.9375\n",
      "0.8125\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.8125\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.8125\n",
      "0.8125\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.8125\n",
      "0.9375\n",
      "0.8125\n",
      "0.9375\n",
      "0.8125\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.8125\n",
      "0.9375\n",
      "0.8125\n",
      "0.9375\n",
      "0.9375\n",
      "0.8125\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.8125\n",
      "0.9375\n",
      "0.8125\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n"
     ]
    }
   ],
   "source": [
    "X_train_test, X_test_test, y_train_test, y_test_test = train_test_split(iris.data,iris.target,test_size=0.85, random_state=11)\n",
    "for i in range(100) :\n",
    "    dt_clf_test = DecisionTreeClassifier(random_state=None)\n",
    "    dt_clf_test.fit(X_train_test,y_train_test)\n",
    "    pred_test = dt_clf_test.predict(X_test_test)\n",
    "    print(accuracy_score(y_test_test,pred_test))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93b3bc21-d7b1-4d5d-83c3-207c5237952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff135745-1df3-4694-a19a-9609701d1719",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델을 파일로 저장\n",
    "f = open('model','wb')\n",
    "# dump dumps 차이 상기\n",
    "pickle.dump(dt_clf,f)\n",
    "# 만든 모델을 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4acdd730-321f-472c-9af5-8d57e4e52a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610981b1-c7ce-41c7-8e73-d6b04a799f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일을 모댈로 불러오기\n",
    "f = open('model','rb')\n",
    "model = pickle.load(f)\n",
    "f.close()\n",
    "model.predict([[4.67,3.9,2,0.4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5975756-beee-46b1-9640-517ce6d8c176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76737196-aac1-4902-861d-cf1b8b036b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서플로우는 자체적인 저장 기능을 제공\n",
    "# 저장 기능이 없더라도, pickle api 를 이용해 파일로 저장 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e72f871-4c4e-47d0-94dc-861ffdb7b78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef21190b-fdaa-4ab8-95c3-64ebafb6235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1cac1e56-3742-4365-be42-fc4a253ab995",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "256b153c-b122-402c-9874-ad14f8fe630c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39bcd787-77ea-443c-b928-2df2e75d6cce",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('data', array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])), ('target', array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])), ('frame', None), ('target_names', array(['setosa', 'versicolor', 'virginica'], dtype='<U10')), ('DESCR', '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...'), ('feature_names', ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']), ('filename', 'iris.csv'), ('data_module', 'sklearn.datasets.data')])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d383be7b-a49b-4453-97ac-04fcd07bcf72",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), None, array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...', ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'iris.csv', 'sklearn.datasets.data'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9256ec96-1619-4eda-b325-84077e839889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(iris.data,iris.target)\n",
    "pred = dt_clf.predict(iris.data)\n",
    "accuracy_score(iris.target,pred)\n",
    "# 학습 데이터 = 테스트 데이터\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5ac46850-54ab-4d70-8ddc-ac28b15d090f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class DecisionTreeClassifier(ClassifierMixin, BaseDecisionTree):\n",
      "    \"\"\"A decision tree classifier.\n",
      "\n",
      "    Read more in the :ref:`User Guide <tree>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
      "        The function to measure the quality of a split. Supported criteria are\n",
      "        \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      "\n",
      "    splitter : {\"best\", \"random\"}, default=\"best\"\n",
      "        The strategy used to choose the split at each node. Supported\n",
      "        strategies are \"best\" to choose the best split and \"random\" to choose\n",
      "        the best random split.\n",
      "\n",
      "    max_depth : int, default=None\n",
      "        The maximum depth of the tree. If None, then nodes are expanded until\n",
      "        all leaves are pure or until all leaves contain less than\n",
      "        min_samples_split samples.\n",
      "\n",
      "    min_samples_split : int or float, default=2\n",
      "        The minimum number of samples required to split an internal node:\n",
      "\n",
      "        - If int, then consider `min_samples_split` as the minimum number.\n",
      "        - If float, then `min_samples_split` is a fraction and\n",
      "          `ceil(min_samples_split * n_samples)` are the minimum\n",
      "          number of samples for each split.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for fractions.\n",
      "\n",
      "    min_samples_leaf : int or float, default=1\n",
      "        The minimum number of samples required to be at a leaf node.\n",
      "        A split point at any depth will only be considered if it leaves at\n",
      "        least ``min_samples_leaf`` training samples in each of the left and\n",
      "        right branches.  This may have the effect of smoothing the model,\n",
      "        especially in regression.\n",
      "\n",
      "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "        - If float, then `min_samples_leaf` is a fraction and\n",
      "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "          number of samples for each node.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for fractions.\n",
      "\n",
      "    min_weight_fraction_leaf : float, default=0.0\n",
      "        The minimum weighted fraction of the sum total of weights (of all\n",
      "        the input samples) required to be at a leaf node. Samples have\n",
      "        equal weight when sample_weight is not provided.\n",
      "\n",
      "    max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
      "        The number of features to consider when looking for the best split:\n",
      "\n",
      "            - If int, then consider `max_features` features at each split.\n",
      "            - If float, then `max_features` is a fraction and\n",
      "              `int(max_features * n_features)` features are considered at each\n",
      "              split.\n",
      "            - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      "            - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "            - If \"log2\", then `max_features=log2(n_features)`.\n",
      "            - If None, then `max_features=n_features`.\n",
      "\n",
      "        Note: the search for a split does not stop until at least one\n",
      "        valid partition of the node samples is found, even if it requires to\n",
      "        effectively inspect more than ``max_features`` features.\n",
      "\n",
      "    random_state : int, RandomState instance or None, default=None\n",
      "        Controls the randomness of the estimator. The features are always\n",
      "        randomly permuted at each split, even if ``splitter`` is set to\n",
      "        ``\"best\"``. When ``max_features < n_features``, the algorithm will\n",
      "        select ``max_features`` at random at each split before finding the best\n",
      "        split among them. But the best found split may vary across different\n",
      "        runs, even if ``max_features=n_features``. That is the case, if the\n",
      "        improvement of the criterion is identical for several splits and one\n",
      "        split has to be selected at random. To obtain a deterministic behaviour\n",
      "        during fitting, ``random_state`` has to be fixed to an integer.\n",
      "        See :term:`Glossary <random_state>` for details.\n",
      "\n",
      "    max_leaf_nodes : int, default=None\n",
      "        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
      "        Best nodes are defined as relative reduction in impurity.\n",
      "        If None then unlimited number of leaf nodes.\n",
      "\n",
      "    min_impurity_decrease : float, default=0.0\n",
      "        A node will be split if this split induces a decrease of the impurity\n",
      "        greater than or equal to this value.\n",
      "\n",
      "        The weighted impurity decrease equation is the following::\n",
      "\n",
      "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "                                - N_t_L / N_t * left_impurity)\n",
      "\n",
      "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "\n",
      "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "        if ``sample_weight`` is passed.\n",
      "\n",
      "        .. versionadded:: 0.19\n",
      "\n",
      "    class_weight : dict, list of dict or \"balanced\", default=None\n",
      "        Weights associated with classes in the form ``{class_label: weight}``.\n",
      "        If None, all classes are supposed to have weight one. For\n",
      "        multi-output problems, a list of dicts can be provided in the same\n",
      "        order as the columns of y.\n",
      "\n",
      "        Note that for multioutput (including multilabel) weights should be\n",
      "        defined for each class of every column in its own dict. For example,\n",
      "        for four-class multilabel classification weights should be\n",
      "        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      "        [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      "\n",
      "        The \"balanced\" mode uses the values of y to automatically adjust\n",
      "        weights inversely proportional to class frequencies in the input data\n",
      "        as ``n_samples / (n_classes * np.bincount(y))``\n",
      "\n",
      "        For multi-output, the weights of each column of y will be multiplied.\n",
      "\n",
      "        Note that these weights will be multiplied with sample_weight (passed\n",
      "        through the fit method) if sample_weight is specified.\n",
      "\n",
      "    ccp_alpha : non-negative float, default=0.0\n",
      "        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "        subtree with the largest cost complexity that is smaller than\n",
      "        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      "        :ref:`minimal_cost_complexity_pruning` for details.\n",
      "\n",
      "        .. versionadded:: 0.22\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    classes_ : ndarray of shape (n_classes,) or list of ndarray\n",
      "        The classes labels (single output problem),\n",
      "        or a list of arrays of class labels (multi-output problem).\n",
      "\n",
      "    feature_importances_ : ndarray of shape (n_features,)\n",
      "        The impurity-based feature importances.\n",
      "        The higher, the more important the feature.\n",
      "        The importance of a feature is computed as the (normalized)\n",
      "        total reduction of the criterion brought by that feature.  It is also\n",
      "        known as the Gini importance [4]_.\n",
      "\n",
      "        Warning: impurity-based feature importances can be misleading for\n",
      "        high cardinality features (many unique values). See\n",
      "        :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "\n",
      "    max_features_ : int\n",
      "        The inferred value of max_features.\n",
      "\n",
      "    n_classes_ : int or list of int\n",
      "        The number of classes (for single output problems),\n",
      "        or a list containing the number of classes for each\n",
      "        output (for multi-output problems).\n",
      "\n",
      "    n_features_ : int\n",
      "        The number of features when ``fit`` is performed.\n",
      "\n",
      "        .. deprecated:: 1.0\n",
      "           `n_features_` is deprecated in 1.0 and will be removed in\n",
      "           1.2. Use `n_features_in_` instead.\n",
      "\n",
      "    n_features_in_ : int\n",
      "        Number of features seen during :term:`fit`.\n",
      "\n",
      "        .. versionadded:: 0.24\n",
      "\n",
      "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "        Names of features seen during :term:`fit`. Defined only when `X`\n",
      "        has feature names that are all strings.\n",
      "\n",
      "        .. versionadded:: 1.0\n",
      "\n",
      "    n_outputs_ : int\n",
      "        The number of outputs when ``fit`` is performed.\n",
      "\n",
      "    tree_ : Tree instance\n",
      "        The underlying Tree object. Please refer to\n",
      "        ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
      "        :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
      "        for basic usage of these attributes.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    DecisionTreeRegressor : A decision tree regressor.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The default values for the parameters controlling the size of the trees\n",
      "    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "    unpruned trees which can potentially be very large on some data sets. To\n",
      "    reduce memory consumption, the complexity and size of the trees should be\n",
      "    controlled by setting those parameter values.\n",
      "\n",
      "    The :meth:`predict` method operates using the :func:`numpy.argmax`\n",
      "    function on the outputs of :meth:`predict_proba`. This means that in\n",
      "    case the highest predicted probabilities are tied, the classifier will\n",
      "    predict the tied class with the lowest index in :term:`classes_`.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "\n",
      "    .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
      "\n",
      "    .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
      "           and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
      "\n",
      "    .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
      "           Learning\", Springer, 2009.\n",
      "\n",
      "    .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
      "           https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.datasets import load_iris\n",
      "    >>> from sklearn.model_selection import cross_val_score\n",
      "    >>> from sklearn.tree import DecisionTreeClassifier\n",
      "    >>> clf = DecisionTreeClassifier(random_state=0)\n",
      "    >>> iris = load_iris()\n",
      "    >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
      "    ...                             # doctest: +SKIP\n",
      "    ...\n",
      "    array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n",
      "            0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        *,\n",
      "        criterion=\"gini\",\n",
      "        splitter=\"best\",\n",
      "        max_depth=None,\n",
      "        min_samples_split=2,\n",
      "        min_samples_leaf=1,\n",
      "        min_weight_fraction_leaf=0.0,\n",
      "        max_features=None,\n",
      "        random_state=None,\n",
      "        max_leaf_nodes=None,\n",
      "        min_impurity_decrease=0.0,\n",
      "        class_weight=None,\n",
      "        ccp_alpha=0.0,\n",
      "    ):\n",
      "        super().__init__(\n",
      "            criterion=criterion,\n",
      "            splitter=splitter,\n",
      "            max_depth=max_depth,\n",
      "            min_samples_split=min_samples_split,\n",
      "            min_samples_leaf=min_samples_leaf,\n",
      "            min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
      "            max_features=max_features,\n",
      "            max_leaf_nodes=max_leaf_nodes,\n",
      "            class_weight=class_weight,\n",
      "            random_state=random_state,\n",
      "            min_impurity_decrease=min_impurity_decrease,\n",
      "            ccp_alpha=ccp_alpha,\n",
      "        )\n",
      "\n",
      "    def fit(\n",
      "        self, X, y, sample_weight=None, check_input=True, X_idx_sorted=\"deprecated\"\n",
      "    ):\n",
      "        \"\"\"Build a decision tree classifier from the training set (X, y).\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The training input samples. Internally, it will be converted to\n",
      "            ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "            to a sparse ``csc_matrix``.\n",
      "\n",
      "        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            The target values (class labels) as integers or strings.\n",
      "\n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights. If None, then samples are equally weighted. Splits\n",
      "            that would create child nodes with net zero or negative weight are\n",
      "            ignored while searching for a split in each node. Splits are also\n",
      "            ignored if they would result in any single class carrying a\n",
      "            negative weight in either child node.\n",
      "\n",
      "        check_input : bool, default=True\n",
      "            Allow to bypass several input checking.\n",
      "            Don't use this parameter unless you know what you do.\n",
      "\n",
      "        X_idx_sorted : deprecated, default=\"deprecated\"\n",
      "            This parameter is deprecated and has no effect.\n",
      "            It will be removed in 1.1 (renaming of 0.26).\n",
      "\n",
      "            .. deprecated:: 0.24\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        self : DecisionTreeClassifier\n",
      "            Fitted estimator.\n",
      "        \"\"\"\n",
      "\n",
      "        super().fit(\n",
      "            X,\n",
      "            y,\n",
      "            sample_weight=sample_weight,\n",
      "            check_input=check_input,\n",
      "            X_idx_sorted=X_idx_sorted,\n",
      "        )\n",
      "        return self\n",
      "\n",
      "    def predict_proba(self, X, check_input=True):\n",
      "        \"\"\"Predict class probabilities of the input samples X.\n",
      "\n",
      "        The predicted class probability is the fraction of samples of the same\n",
      "        class in a leaf.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The input samples. Internally, it will be converted to\n",
      "            ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "            to a sparse ``csr_matrix``.\n",
      "\n",
      "        check_input : bool, default=True\n",
      "            Allow to bypass several input checking.\n",
      "            Don't use this parameter unless you know what you do.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        proba : ndarray of shape (n_samples, n_classes) or list of n_outputs \\\n",
      "            such arrays if n_outputs > 1\n",
      "            The class probabilities of the input samples. The order of the\n",
      "            classes corresponds to that in the attribute :term:`classes_`.\n",
      "        \"\"\"\n",
      "        check_is_fitted(self)\n",
      "        X = self._validate_X_predict(X, check_input)\n",
      "        proba = self.tree_.predict(X)\n",
      "\n",
      "        if self.n_outputs_ == 1:\n",
      "            proba = proba[:, : self.n_classes_]\n",
      "            normalizer = proba.sum(axis=1)[:, np.newaxis]\n",
      "            normalizer[normalizer == 0.0] = 1.0\n",
      "            proba /= normalizer\n",
      "\n",
      "            return proba\n",
      "\n",
      "        else:\n",
      "            all_proba = []\n",
      "\n",
      "            for k in range(self.n_outputs_):\n",
      "                proba_k = proba[:, k, : self.n_classes_[k]]\n",
      "                normalizer = proba_k.sum(axis=1)[:, np.newaxis]\n",
      "                normalizer[normalizer == 0.0] = 1.0\n",
      "                proba_k /= normalizer\n",
      "                all_proba.append(proba_k)\n",
      "\n",
      "            return all_proba\n",
      "\n",
      "    def predict_log_proba(self, X):\n",
      "        \"\"\"Predict class log-probabilities of the input samples X.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The input samples. Internally, it will be converted to\n",
      "            ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "            to a sparse ``csr_matrix``.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        proba : ndarray of shape (n_samples, n_classes) or list of n_outputs \\\n",
      "            such arrays if n_outputs > 1\n",
      "            The class log-probabilities of the input samples. The order of the\n",
      "            classes corresponds to that in the attribute :term:`classes_`.\n",
      "        \"\"\"\n",
      "        proba = self.predict_proba(X)\n",
      "\n",
      "        if self.n_outputs_ == 1:\n",
      "            return np.log(proba)\n",
      "\n",
      "        else:\n",
      "            for k in range(self.n_outputs_):\n",
      "                proba[k] = np.log(proba[k])\n",
      "\n",
      "            return proba\n",
      "\n",
      "    @deprecated(  # type: ignore\n",
      "        \"The attribute `n_features_` is deprecated in 1.0 and will be removed \"\n",
      "        \"in 1.2. Use `n_features_in_` instead.\"\n",
      "    )\n",
      "    @property\n",
      "    def n_features_(self):\n",
      "        return self.n_features_in_\n",
      "\n",
      "    def _more_tags(self):\n",
      "        return {\"multilabel\": True}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(DecisionTreeClassifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da626e0e-c394-496c-97d4-669138341ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c635e3-aa40-4992-b4ad-b5df33fe0a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 사용하는 데이터, 검증에 사용하는 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb7284d-14a7-4824-87ad-1effe2fd07d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "8988"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
